{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四章：训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前几章讨论的几个模型都不知道具体的实施细节。\n",
    "\n",
    "* 一：这一章，将从**线性回归模型**开始。介绍两种非常不同的训练模型的方法。  \n",
    "    * 1、通过“闭式”方程——直接计算出最适合训练集的模型参数（也就是使训练集上的成本函数最小化的模型参数）  \n",
    "    * 2、使用迭代优化的方法，即梯度下降（`GD`），逐渐调整模型参数直至训练集上的成本函数调至最低，最终趋同于第一种方法计算出来的模型参数。也会研究几个梯度下降的变体，包括批量梯度下降、小批量梯度下降以及随机梯度下降。  \n",
    "\n",
    "* 二：进行多项式回归的讨论，这是一个更为复杂的模型，更适合非线性数据集。由于该模型的参数比线性模型更多，因此更容易造成对训练数据过度拟合，我们将使用学习曲线来分辨这种情况是否发生，然后再介绍几种正则化方法，降低过度拟合训练数据的风险。\n",
    "\n",
    "* 三：学习两种经常用于分类任务的模型：`Logistic`回归和`Softmax`回归。\n",
    "\n",
    "了解这些系统如何工作非常有帮助。针对你的任务，它有助于快速定位到合适的模型，正确的训练算法，以及一套适当的超参数。不仅如此，后期还能更高效的执行错误调试和错误分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "# CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\") # , CHAPTER_ID\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "# 存图片时，要先存（save_fig），再展示（show）\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第1章中，学过一个简单的生活满意度的回归模型：life_satisfaction= $\\theta_{0}+\\theta_{1} \\times \\mathrm{GDP}_{-}$ per_capita\n",
    "\n",
    "这个模型就是输入特征`GDP_per_capita`的线性函数，$\\theta_{0}$和$\\theta_{1}$是模型的参数。  \n",
    "更为概括的说，线性模型就是对输入特征加权求和，再加上一个偏置项（也称为截距项）的常数。  \n",
    "公式如下：  \n",
    "<center>$\\hat{y}=\\theta_{0}+\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\cdots+\\theta_{n} x_{n}$</center>\n",
    "\n",
    "* $\\hat{y}$是预测值  \n",
    "\n",
    "* $n$是特征的数量  \n",
    "\n",
    "* $x_{\\mathrm{i}}$是第$i$个特征值\n",
    "\n",
    "* $\\theta_{j}$是第$j$个模型参数（包括偏执项$\\theta_{0}$以及特征权重$\\theta_{1}$,$\\theta_{2}$,···,$\\theta_{n}$）\n",
    "\n",
    "这也可以用更为简单的向量化形式表达，公式如下：  \n",
    "<center>$\\hat{y}=h_{\\theta}(\\mathbf{x})=\\theta^{T} \\cdot \\mathbf{x}$</center>\n",
    "\n",
    "* $\\theta$是模型的参数向量，包括偏置项$\\theta_{0}$以及特征权重$\\theta_{1}$到$\\theta_{n}$\n",
    "\n",
    "* $\\theta^{T}$是$\\theta$的转置向量\n",
    "\n",
    "* $\\mathbf{X}$是实例的特征向量，包括从$x_{0}$到$x_{n}$，$x_{0}$永远为1\n",
    "\n",
    "* $\\theta^{T} \\cdot \\mathbf{x}$是$\\theta^{T}$和$\\mathbf{X}$的点积\n",
    "\n",
    "* $h_{\\theta}$是使用模型参数$\\theta$的假设函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型就是设置模型参数直到模型最适应训练集的过程。要达到这个目的，首先需要知道怎么衡量模型对训练数据的拟合程度是好还是差。在第2章中，我们了解到回归模型最常见的性能指标是均方根误差（`RMSE`）。因此，在训练线性回归模型时，需要找到最小化`RMSE`的$\\theta$值，在实践中，将均方误差（`MSE`）最小化比最小化`RMSE`更为简单，二者效果相同（因为使函数最小化的值，同样也使其平方根最小。）<sup>注1</sup>\n",
    "\n",
    "在训练集$\\mathbf{X}$上，使用如下公式计算线性回归的`MSE`，$h_{\\theta}$为假设函数  \n",
    "<center>$\\operatorname{MSE}\\left(\\mathbf{X}, h_{\\theta}\\right)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(\\theta^{T} \\cdot \\mathbf{x}^{(i)}-y^{(i)}\\right)^{2}$</center>\n",
    "\n",
    "注1：通常情况下，学习算法优化的函数都与评估最终模型时使用的性能指标函数不同。这可能是1、前者更容易计算；2、前者具有某些后者缺乏的差异属性；3、想在训练期间约束模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提供一个闭式解方法（直接得出结果的数学方程--标准方程）\n",
    "\n",
    "<center>$\\widehat{\\theta}=\\left(X^{T} \\cdot X\\right)^{-1} \\cdot X^{T} \\cdot \\mathrm{y}$</center>\n",
    "\n",
    "* $\\widehat{\\theta}$是使成本函数最小的${\\theta}$值\n",
    "* $y$是包含$y^{(1)}$到$y^{(m)}$的目标值向量\n",
    "\n",
    "生成一些线性数据来测试这个公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# 接下来计算theta_hat，使用np.linalg中的inv()函数来对矩阵求逆，并用dot()方法计算矩阵的内积\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用来生成数据的函数是$y=4+3x_{0}+高斯噪声$，期望得到的${\\theta_{0}}=4$，${\\theta_{1}}=3$，实际得到的结果是${\\theta_{0}}=3.815$，${\\theta_{1}}=3.184$。这两个结果非常接近（因为存在高斯噪声的缘故不可能完全一样）。\n",
    "\n",
    "下面用$\\widehat{\\theta}$做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 预测随机生成的线性数据\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制模型的预测情况\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "save_fig(\"预测随机生成的线性数据\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21509616]), array([[2.77011339]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit-Learn 的等效代码：\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [9.75532293]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准方程的计算复杂度通常为$O(n^{2.4})$到$O(n^{3})$之间。所以，当特征向量翻倍时，计算时间大约在$2^{2.4}=5.3$到$2^{3}=8$倍之间。\n",
    "\n",
    "好的一方面是，对于训练集中的实例数量来说，方程是线性的，所以能够有效地处理大量的训练集（只要内存足够）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，看几个适合特征数或者训练实例数量大到内存无法满足要求时的模型训练方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降能够为大范围的问题找到最优解：通过测量参数向量${\\theta}$相关的误差函数的局部梯度，并不断沿着降低梯度的方向调整，直到梯度降为0，到达最小值。\n",
    "\n",
    "具体来说，首先使用一个随机的${\\theta}$值（随机初始化），然后逐步改进，每次踏出一步，每一步都尝试降低一点成本函数，知道算法收敛出一个最小值。\n",
    "<img src=\"./images/Book/梯度下降.jpg\" width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "**存在的问题：**\n",
    "* 当学习率太低时，算法需要经过大量迭代才能收敛，将耗费很长时间\n",
    "<img src=\"./images/Book/学习率太低.jpg\" width=\"50%\" height=\"50%\"></img>\n",
    "* 如果学习率太高，会导致算法发散，值越来越大。\n",
    "<img src=\"./images/Book/学习率太高.jpg\" width=\"50%\" height=\"50%\"></img>\n",
    "* 并不是所有的成本函数都像一个漂亮的碗，不同形状的成本函数会收敛到一个局部最小值而不是全局最小值。\n",
    "<img src=\"./images/Book/梯度下降陷阱.jpg\" width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "    幸好，线性回归模型的成本函数恰好是一个凸函数，也就是说不存在局部最小，只有一个全局最小值。同时他也是一个连续函数，所以斜率不会产生陡峭的变化。**结论**：即便是乱走，梯度下降都可以趋近到全局最小值。\n",
    "* 成本函数虽然是碗状的，但如果不同特征的尺寸差别巨大，那它可能是一个非常细小的碗，这将导致收敛缓慢。**所以要保证所有特征值的大小比例都差不多**\n",
    "<img src=\"./images/Book/特征值缩放和特征值无缩放的梯度下降.jpg\" width=\"50%\" height=\"50%\"></img>\n",
    "* 模型的参数越多，这个空间的维度就越多，搜索就越难。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要实现梯度下降，需要计算每个模型关于参数$\\theta_{j}$的成本函数的梯度。\n",
    "\n",
    "* 成本函数的偏导数计算公式：\n",
    "\n",
    "<center>$\\frac{\\partial}{\\partial \\theta_{j}} \\operatorname{MSE}(\\theta)=\\frac{2}{m} \\sum_{i=1}^{m}\\left(\\theta^{T} \\cdot \\mathbf{x}^{(i)}-y^{(i)}\\right) x_{j}^{(i)}$</center>\n",
    "\n",
    "* 成本函数的梯度向量：\n",
    "\n",
    "<center>$\\nabla_{\\theta} \\operatorname{MSE}(\\theta)=\\left(\\begin{array}{c}\\frac{\\partial}{\\partial \\theta_{0}} \\operatorname{MSE}(\\theta) \\\\ \\frac{\\partial}{\\partial \\theta_{1}} \\operatorname{MSE}(\\theta) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial \\theta_{n}} \\operatorname{MSE}(\\theta)\\end{array}\\right)=\\frac{2}{m} \\mathbf{X}^{T} \\cdot(\\mathbf{X} \\cdot \\theta-\\mathbf{y})$</center>\n",
    "\n",
    "**注意**，批量梯度下降在计算梯度下降的每一步时，都是基于完整的训练集，因此，面对非常庞大的训练集时，算法会变得极慢。但是，相较于标准方程，梯度下降算法随特征向量扩展的表现更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦有了梯度向量，哪个点向上，就朝反方向下坡，也就是从$\\theta$中减去$\\nabla_{\\theta} \\operatorname{MSE}(\\theta)$。这时用梯度向量乘以$\\eta$确定下坡步长的大小。\n",
    "\n",
    "* 梯度下降步长：\n",
    "<center>$\\theta^{(\\text {next step })}=\\theta-\\eta \\nabla_{\\theta} \\operatorname{MSE}(\\theta)$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看这个算法的快速实现\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试不同的学习率：\n",
    "<img src=\"./images/Book/不同学习率的梯度下降.jpg\" width=\"70%\" height=\"70%\"></img>\n",
    "\n",
    "从图中看出，左图的学习率太低，算法最终可以找到解决方法，但是要较长时间。中间的学习率正好。而右边的学习率太高导致算法发散，找不到解决方法。\n",
    "\n",
    "要找到合适的学习率，可以使用网格搜索，但是需要限制迭代次数，这样网格搜索可以淘汰掉那些收敛耗时太长的模型。一个简单方法是：开始设置一个非常大的迭代次数，当梯度向量的值变得很微小时中断算法---也就是当它的范数变得低于$\\mathcal{E}$（容差）时，这时梯度下降（几乎）已经到达了最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>收敛率</center>\n",
    "成本函数为凸函数，且斜率没有陡峭变化时（如MSE成本函数），通过批量梯度下降可以看出一个固定的学习率有一个收敛率，为$O\\left(\\frac{1}{\\text { iterations }}\\right)$。所以，如果将容差$\\mathcal{E}$缩小为原来的1/10（以获得更精确的解），算法将运行10倍的迭代次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
